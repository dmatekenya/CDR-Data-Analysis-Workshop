{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Individual User Attributes\n",
    "----------------------------------------------------------\n",
    "\n",
    "In this notebook, we demonstrate how to preprocess CDRs and also generate mobility variables for individual subscribers. A few notes below:\n",
    "- **Subscriber/user identification:** in order to keep track of a user, we need to have a unique identfier. we use telephone number for this purpose. We use both the calling and incomign telephone numbers.\n",
    "- **Anonymizing the user identification:** You will note that the telephone numbers have been anonymized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python setup\n",
    "Import all the required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType\n",
    "import seaborn as sns\n",
    "import d5_solutions as sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def preprocess_cdrs_using_spark(file_or_folder=None, number_of_users_to_sample=None,\n",
    "                                output_csv=None, date_format='%Y%m%d%H%M%S',\n",
    "                                debug_mode=True, loc_file=None, save_to_csv=False):\n",
    "    \"\"\"\n",
    "    In this function, we perfom some basic preprocessing such as below:\n",
    "    1. rename columns\n",
    "    2. change some data types\n",
    "    3. Add location details\n",
    "    Eventually, we will sample the data to use for our analysis\n",
    "    :param data_folder:\n",
    "    :param output_csv_for_sample_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # create SparkSession object\n",
    "    spark = SparkSession.builder.master(\"local[8]\").appName(\"data_processor\").getOrCreate()\n",
    "\n",
    "    # read data with spark\n",
    "    df = spark.read.csv(path=file_or_folder, header=True)\n",
    "\n",
    "    # repartition to speed up\n",
    "    df = df.repartition(10)\n",
    "\n",
    "    # if just testing/debugging, pick only a small dataset\n",
    "    if debug_mode:\n",
    "        dfs = df.sample(fraction=0.01)\n",
    "        df = dfs\n",
    "\n",
    "    # rename columns to remove space and replace with underscore\n",
    "    df2 = (df.withColumnRenamed(\"cdr datetime\", \"cdrDatetime\")\n",
    "        .withColumnRenamed(\"calling phonenumber2\", \"phoneNumber\")\n",
    "        .withColumnRenamed(\"last calling cellid\", \"cellId\")\n",
    "        .withColumnRenamed(\"call duration\", \"cellDuration\"))\n",
    "\n",
    "    # drop cdr type column\n",
    "    df3 = df2.drop('cdr type')\n",
    "\n",
    "    # Use Spark UDF to add date and datetime\n",
    "    add_datetime = udf(lambda x: datetime.strptime(x, date_format), TimestampType())\n",
    "    add_date = udf(lambda x: datetime.strptime(x, date_format), DateType())\n",
    "\n",
    "    # create timestamp\n",
    "    df4 = df3.withColumn('datetime', add_datetime(col('cdrDatetime')))\n",
    "    df5 = df4.withColumn('date', add_date(col('cdrDatetime')))\n",
    "\n",
    "    # lets make sure we dont have any null phoneNumbers\n",
    "    df6 = df5.filter(df5['phoneNumber'].isNotNull())\n",
    "\n",
    "    # Lets merge with location details using cellId from CDRs and also\n",
    "    # cellID on the other\n",
    "    dfLoc = pd.read_csv(loc_file)\n",
    "    dfLoc.rename(columns={'cell_id': 'cellId'}, inplace=True)\n",
    "    sdfLoc = spark.createDataFrame(dfLoc)\n",
    "    df7 = df6.join(sdfLoc, on='cellId', how='inner')\n",
    "    \n",
    "    return df7\n",
    "\n",
    "#     # select nsample users to work with\n",
    "#     all_users = df7.select('phoneNumber').distinct().collect()\n",
    "\n",
    "#     # randomly select users using filter statement\n",
    "#     random_user_numbers = [i['phoneNumber'] for i in random.choices(all_users, k=number_of_users_to_sample)]\n",
    "\n",
    "#     # select only our random user data\n",
    "#     dfu = df7.filter(df7['phoneNumber'].isin(random_user_numbers))\n",
    "\n",
    "#     # save to CSV if necessary\n",
    "#     if save_to_csv:\n",
    "#         dfu.coalesce(1).write.csv(path=output_csv, header=True)\n",
    "#     else:\n",
    "#         return dfu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def combine_selected_csv_files(folder_with_csv_files=None, number_to_save=None, out_csv_file=None):\n",
    "    \"\"\"\n",
    "    Save a sample of the small CSV files into a CSV file for exploration.\n",
    "    Please test this with very few files to avoid wasting time\n",
    "    :param folder_with_csv_files:\n",
    "    :param number_to_save:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # get a list of CSV file using os module listdir() function\n",
    "    files = os.listdir(folder_with_csv_files)\n",
    "    \n",
    "    # create a list to hold pandas dataframes\n",
    "    df_lst = []\n",
    "    \n",
    "    #create a counter variable whcih will help you stop\n",
    "    # the loop when you reach the required number of files\n",
    "    cnt = 0\n",
    "    for f in files:\n",
    "        if f.endswith('csv'):\n",
    "            fpath = os.path.join(folder_with_csv_files, f)\n",
    "            df = pd.read_csv(fpath)\n",
    "            # append this df to the list of dfs above\n",
    "            df_lst.append(df)\n",
    "    \n",
    "            # increment the counter variable\n",
    "            cnt += 1\n",
    "    \n",
    "            # stop the loop using break statement when you have\n",
    "            # processes the required number of files\n",
    "            # as defined by number_to_save\n",
    "            if cnt == number_to_save:\n",
    "                break\n",
    "    \n",
    "    # use pandas function concat() like this: pd.concat()\n",
    "    # to concatenate all the dfs in the list\n",
    "    df = pd.concat(df_lst)\n",
    "    \n",
    "    # save your new dataframe\n",
    "    df.to_csv(out_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def explore_data(df=None, output_plot_file=None, output_heatmap=None):\n",
    "    \"\"\"\n",
    "    For quick examination of user activity, lets generate\n",
    "    user call count and do a simple plot.\n",
    "    \"\"\"\n",
    "    # Number of days in data\n",
    "    dates_rows = df.select('date').distinct().collect()\n",
    "    sorted_dates = sorted([i['date'] for i in dates_rows])\n",
    "    diff = sorted_dates[-1] - sorted_dates[0]\n",
    "    num_days = diff.days\n",
    "\n",
    "    # call count by hour\n",
    "    add_hr = udf(lambda x: x.hour, IntegerType())\n",
    "    add_wkday = udf(lambda x: x.weekday(), IntegerType())\n",
    "    day_dict = {0: 'Mon', '1': 'Tue', '2': 'Wed', 3: 'Thurs', 4: 'Frid', 5: 'Sat', 6: 'Sun'}\n",
    "\n",
    "    dfHr = df.withColumn('hr', add_hr(col('datetime')))\n",
    "    dfHr2 = dfHr.withColumn('wkday', add_wkday(col('datetime')))\n",
    "    dfWkDay = dfHr2.groupBy('wkday', 'hr').count().toPandas()\n",
    "    dfWkDay['weekDay'] = dfWkDay.apply(add_weekdays, args=(day_dict,), axis=1)\n",
    "    dfWkDay.drop(labels=['wkday'], axis=1, inplace=True)\n",
    "    dfWkDayPivot = dfWkDay.pivot(index='weekDay', columns='hr', values='count')\n",
    "    d = dfWkDayPivot.reset_index()\n",
    "    ax = sns.heatmap(d)\n",
    "    ax.get_figure().savefig(output_heatmap)\n",
    "\n",
    "    # group user and count number of events\n",
    "    # convert resulting spark dataframe to pandas\n",
    "    dfGroup = df.groupBy('phoneNumber').count().toPandas()\n",
    "\n",
    "    # create a distribution plot of user call count using\n",
    "    # seaborn\n",
    "    ax = sns.distplot(dfGroup['count'])\n",
    "\n",
    "    # save plot as png file\n",
    "    ax.get_figure().savefig(output_plot_file)\n",
    "\n",
    "    # report average number calls per day for each user\n",
    "    dfGroupDay = df.groupBy('phoneNumber', 'date').count().toPandas()\n",
    "\n",
    "    # get mean and median\n",
    "    mean = dfGroupDay['count'].mean()\n",
    "    median = dfGroupDay['count'].median()\n",
    "\n",
    "    # data duration\n",
    "    return mean, median, num_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder = \"/Users/dmatekenya/cdrsAfricellHashed/\"\n",
    "outputsFolder = \"/Users/dmatekenya/Google-Drive/teachingAndLearning/cdrTrainingFreetown/outputs/\"\n",
    "locFile = \"../data/africell-loc-with-admin-attributes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Exploration\n",
    "Like in any other data processing and analysis work, before we start generating the useful variables from CDRs,\n",
    "they are key preprocessing steps to be done. The preprocessing steps arent universal but rather they depend \n",
    "on the data you start with as well as the analysis objectives. For our purpose, we do the following:\n",
    "- Drop unnecessary columns\n",
    "- Format time stamps\n",
    "- Rename columns\n",
    "- Drop events without phonenumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field admin4Name: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3a4e4ca5b405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                   \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%Y%m%d%H%M%S'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   \u001b[0mloc_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocFile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                  save_to_csv=False, debug_mode=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d73b6e754d03>\u001b[0m in \u001b[0;36mpreprocess_cdrs_using_spark\u001b[0;34m(file_or_folder, number_of_users_to_sample, output_csv, date_format, debug_mode, loc_file, save_to_csv)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mdfLoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mdfLoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'cell_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cellId'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0msdfLoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfLoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mdf7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdfLoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cellId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    340\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    341\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1132\u001b[0m                                                   name=new_name(f.name)))\n\u001b[0;32m-> 1133\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1134\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1132\u001b[0m                                                   name=new_name(f.name)))\n\u001b[0;32m-> 1133\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1134\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not merge type %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: field admin4Name: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "dfs = preprocess_cdrs_using_spark(file_or_folder=dataFolder, number_of_users_to_sample=1000, \n",
    "                                  date_format='%Y%m%d%H%M%S', \n",
    "                                  loc_file=locFile,\n",
    "                                 save_to_csv=False, debug_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, median, num_days, hrDayCnts = sol.explore_data(df=dfs, output_plot_file=users_call_cnt_plt , \n",
    "             output_heatmap=heatMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cellId='20532.0', cdrDatetime='20180711204136', cellDuration=None, phoneNumber='8204330690229196471', datetime=datetime.datetime(2018, 7, 11, 20, 41, 36), date=datetime.date(2018, 7, 11), site_id='s91', lon=28.885225, lat=-19.060772)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('8204330690229196471',\n",
       " Row(cellId='20532.0', cdrDatetime='20180711204136', cellDuration=None, phoneNumber='8204330690229196471', datetime=datetime.datetime(2018, 7, 11, 20, 41, 36), date=datetime.date(2018, 7, 11), site_id='s91', lon=28.885225, lat=-19.060772))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpByUser = dfs.rdd.map(lambda x: (x['phoneNumber'], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('217873087185206285', <pyspark.resultiterable.ResultIterable at 0x1a30c7e828>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpByUser2 = grpByUser.groupByKey()\n",
    "grpByUser2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpByUser = dfs.rdd.map(lambda x: (x['phoneNumber'], x))\n",
    "grpByUser2 = grpByUser.groupByKey()\n",
    "\n",
    "# select nsample users to work with\n",
    "all_users = df7.select('phoneNumber').distinct().collect()\n",
    "\n",
    "# randomly select users using filter statement\n",
    "random_user_numbers = [i['phoneNumber'] for i in random.choices(all_users, k=number_of_users_to_sample)]\n",
    "\n",
    "# select only our random user data\n",
    "\n",
    "dfu = df7.filter(df7['phoneNumber'].isin(random_user_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select nsample users to work with\n",
    "all_users = dfs.select('phoneNumber').distinct().collect()\n",
    "\n",
    "# randomly select users using filter statement\n",
    "random_user_numbers = [i['phoneNumber'] for i in random.choices(all_users, k=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfu = grpByUser2.filter(lambda x: x[0] in random_user_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5030518197241310850',\n",
       " <pyspark.resultiterable.ResultIterable at 0x1a421a5198>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfu.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = dfu.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
